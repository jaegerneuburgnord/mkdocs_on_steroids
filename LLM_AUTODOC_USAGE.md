# MkDocs LLM AutoDoc Plugin - Verwendung

## âœ… Status: Plugin erfolgreich konfiguriert!

Das LLM AutoDoc Plugin wurde erfolgreich installiert und konfiguriert.

## Aktuelle Konfiguration

Ihr Plugin ist konfiguriert fÃ¼r:
- **LLM Provider**: OpenAI-kompatibler Server
- **Server URL**: `http://localhost:11434/v1`
- **C++ Projekt**: `./cpp-project`
- **Dokumentationsebenen**: High-Level, Mid-Level, Detailed-Level (alle aktiviert)
- **Code-Review**: Aktiviert
- **Caching**: Aktiviert

## Vor der ersten Verwendung

### 1. LLM Server starten

Stellen Sie sicher, dass Ihr lokaler LLM-Server lÃ¤uft:

**FÃ¼r Ollama:**
```bash
ollama serve
```

**FÃ¼r LM Studio:**
1. LM Studio Ã¶ffnen
2. Ein Modell laden (z.B. CodeLlama, DeepSeek Coder, Mistral)
3. Server starten: Developer â†’ Start Server (Port 11434)

**FÃ¼r Ollama mit anderem Port:**
Falls Ihr Server auf einem anderen Port lÃ¤uft, passen Sie die `llm_base_url` in `mkdocs.yml` an.

### 2. Modellname anpassen

Ã–ffnen Sie `mkdocs.yml` und Ã¤ndern Sie den Modellnamen:
```yaml
llm_model: 'local-model'  # Ã„ndern Sie dies!
```

Zu Ihrem tatsÃ¤chlichen Modellnamen, z.B.:
- `'llama3'`
- `'codellama'`
- `'deepseek-coder'`
- `'mistral'`
- Etc.

## Dokumentation generieren

### Erste Generation (vollstÃ¤ndig)

```bash
mkdocs build
```

Das Plugin wird:
1. Ihr C++ Projekt analysieren (`./cpp-project/`)
2. Mit dem LLM-Server kommunizieren
3. Drei Ebenen von Dokumentation generieren:
   - `docs/generated/` - High-Level (ProjektÃ¼bersicht, Architektur)
   - `docs/generated/modules/` - Mid-Level (Module, Klassen)
   - `docs/generated/api/` - Detailed-Level (API-Referenz)
4. Code-Review Berichte erstellen

### Nur geÃ¤nderte Dateien neu generieren

Das Plugin cached automatisch! Beim nÃ¤chsten Build werden nur geÃ¤nderte Dateien neu dokumentiert:

```bash
# C++ Datei Ã¤ndern
echo "// Updated" >> cpp-project/src/myfile.cpp

# Nur die geÃ¤nderte Datei wird neu dokumentiert
mkdocs build
```

### Alles neu generieren (Cache ignorieren)

```yaml
# In mkdocs.yml:
plugins:
  - llm-autodoc:
      force_regenerate: true  # Cache wird ignoriert
```

## Dokumentation ansehen

```bash
mkdocs serve
```

Ã–ffnen Sie dann http://localhost:8000 in Ihrem Browser.

## Konfiguration anpassen

### Andere LLM-Provider verwenden

Sie kÃ¶nnen jederzeit das Setup-Script erneut ausfÃ¼hren:

```bash
python setup_llm_autodoc.py
```

Oder manuell in `mkdocs.yml` Ã¤ndern:

**Anthropic Claude:**
```yaml
llm_provider: 'anthropic'
llm_model: 'claude-3-5-sonnet-20241022'
llm_api_key: !ENV ANTHROPIC_API_KEY
llm_base_url: null  # Nicht benÃ¶tigt
```

**OpenAI GPT-4:**
```yaml
llm_provider: 'openai'
llm_model: 'gpt-4'
llm_api_key: !ENV OPENAI_API_KEY
llm_base_url: null  # Nicht benÃ¶tigt
```

### Dokumentationsebenen selektiv aktivieren

Um Zeit und Kosten zu sparen, kÃ¶nnen Sie einzelne Ebenen deaktivieren:

```yaml
generate_high_level: true      # Projekt-Ãœbersicht (schnell)
generate_mid_level: true       # Modul-Dokumentation (mittel)
generate_detailed_level: false # API-Referenz (langsam) - DEAKTIVIERT
```

### Bestimmte Dateien ausschlieÃŸen

```yaml
exclude_patterns:
  - '**/build/**'
  - '**/third_party/**'
  - '**/test/**'
  - '**/examples/**'
  - '**/deprecated/**'
```

### Parallele LLM-Aufrufe anpassen

```yaml
max_concurrent_llm_calls: 3  # Standard
# Reduzieren bei Rate-Limits: 1
# ErhÃ¶hen fÃ¼r schnellere Builds: 5
```

## Generierte Dateien

Nach dem Build finden Sie:

```
docs/
â”œâ”€â”€ generated/
â”‚   â”œâ”€â”€ 00-getting-started.md     # Projekt-Ãœbersicht
â”‚   â”œâ”€â”€ 01-architecture.md        # Architektur & Diagramme
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ core.md              # Core-Modul Dokumentation
â”‚   â”‚   â”œâ”€â”€ utils.md             # Utils-Modul Dokumentation
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ classes/
â”‚       â”‚   â”œâ”€â”€ myclass.md       # Detaillierte Klassen-Dokumentation
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ functions/
â”‚           â”œâ”€â”€ helpers.md       # Funktions-Dokumentation
â”‚           â””â”€â”€ ...
```

## Cache-Verwaltung

### Cache-Verzeichnis

Der Cache wird gespeichert in `.cache/llm-autodoc/`

### Cache lÃ¶schen

```bash
rm -rf .cache/llm-autodoc
```

Oder in PowerShell:
```powershell
Remove-Item -Recurse -Force .cache\llm-autodoc
```

## Fehlerbehebung

### "No API key provided"

**Problem:** Das Plugin kann nicht mit dem LLM kommunizieren.

**LÃ¶sung:**
1. FÃ¼r lokale Server (Ollama/LM Studio): Stellen Sie sicher, dass `llm_api_key: 'not-needed'` gesetzt ist
2. FÃ¼r Cloud-Provider: Setzen Sie die Umgebungsvariable:
   ```bash
   export ANTHROPIC_API_KEY='your-key'
   # oder
   export OPENAI_API_KEY='your-key'
   ```

### "Failed to initialize LLM provider"

**Problem:** Der LLM-Server ist nicht erreichbar.

**LÃ¶sung:**
1. PrÃ¼fen Sie, ob der Server lÃ¤uft:
   ```bash
   curl http://localhost:11434/v1/models
   ```
2. PrÃ¼fen Sie die `llm_base_url` in `mkdocs.yml`
3. FÃ¼r Ollama: `ollama serve`
4. FÃ¼r LM Studio: Server starten in der GUI

### "C++ project path not found"

**Problem:** Der Pfad zum C++ Projekt ist falsch.

**LÃ¶sung:**
1. PrÃ¼fen Sie `cpp_project_path` in `mkdocs.yml`
2. Der Pfad ist relativ zum mkdocs Hauptverzeichnis
3. Beispiel: Wenn Ihr Projekt in `Z:\mkdocs\cpp-project\` liegt, verwenden Sie `'./cpp-project'`

### Build dauert sehr lange

**Problem:** Das Plugin wartet auf LLM-Antworten.

**LÃ¶sungen:**
1. Reduzieren Sie `max_concurrent_llm_calls`
2. Deaktivieren Sie `generate_detailed_level` fÃ¼r schnellere Builds
3. Nutzen Sie Caching (`enable_cache: true`)
4. SchlieÃŸen Sie Test-Dateien aus

### "Tree-sitter not available"

**Problem:** C++ Parser fehlt (nicht kritisch).

**LÃ¶sung:**
```bash
pip install tree-sitter tree-sitter-cpp
```

Das Plugin funktioniert auch mit dem Fallback-Parser, aber tree-sitter ist genauer.

## Performance-Tipps

### 1. Caching verwenden
```yaml
enable_cache: true  # Standard
force_regenerate: false  # Nur bei Bedarf auf true
```

### 2. Selektive Generation
```yaml
# FÃ¼r schnelle Builds:
generate_high_level: true
generate_mid_level: true
generate_detailed_level: false  # Ãœberspringen
```

### 3. Tests ausschlieÃŸen
```yaml
exclude_patterns:
  - '**/test/**'
  - '**/tests/**'
  - '**/*_test.cpp'
```

### 4. Lokale Modelle nutzen
- Ollama und LM Studio sind kostenlos
- Keine API-Kosten
- Oft schneller als Cloud-APIs bei kleinen Projekten

## Kosten (bei Cloud-Providern)

### Typisches mittelgroÃŸes C++ Projekt (50-100 Dateien):

**Anthropic Claude:**
- Erste vollstÃ¤ndige Generation: ~$2-5
- Inkrementelle Updates: ~$0.10-0.50

**OpenAI GPT-4:**
- Erste vollstÃ¤ndige Generation: ~$10-20
- Inkrementelle Updates: ~$0.50-2

**Ollama/LM Studio:**
- Kostenlos! ðŸŽ‰

## FÃ¼r andere Projekte verwenden

```bash
# 1. Script kopieren
cp setup_llm_autodoc.py /pfad/zu/anderem/projekt/

# 2. Im neuen Projekt ausfÃ¼hren
cd /pfad/zu/anderem/projekt/
python setup_llm_autodoc.py

# 3. Plugin installieren
cd plugins/mkdocs-llm-autodoc
pip install -e .

# 4. Dokumentation generieren
mkdocs build
```

## Weitere Informationen

- **Plugin-Dokumentation**: `plugins/mkdocs-llm-autodoc/README.md`
- **Quick Start**: `plugins/mkdocs-llm-autodoc/QUICKSTART.md`
- **Setup-Script**: `SETUP_README.md`

## Support

Bei Problemen:
1. PrÃ¼fen Sie diese Datei
2. Lesen Sie die Plugin-Dokumentation
3. PrÃ¼fen Sie die MkDocs Build-Logs (`mkdocs build --verbose`)
4. Testen Sie die LLM-Verbindung manuell

---

**Viel Erfolg mit Ihrer automatisch generierten C++ Dokumentation! ðŸš€**
