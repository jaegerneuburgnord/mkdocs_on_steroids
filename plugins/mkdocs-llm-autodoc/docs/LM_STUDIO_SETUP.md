# LM Studio Setup Guide

LM Studio ist eine benutzerfreundliche Desktop-Anwendung f√ºr lokale LLM-Inferenz mit OpenAI-kompatibler API.

## Warum LM Studio?

- ‚úÖ **Benutzerfreundliche GUI** - Keine Kommandozeile n√∂tig
- ‚úÖ **Kostenlos** - Keine API-Kosten
- ‚úÖ **Offline** - Volle Datenkontrolle
- ‚úÖ **Viele Modelle** - Gro√üe Auswahl an GGUF-Modellen
- ‚úÖ **OpenAI-kompatibel** - Einfache Integration
- ‚úÖ **Cross-Platform** - Windows, macOS, Linux

## Installation

### 1. LM Studio herunterladen

Besuche [lmstudio.ai](https://lmstudio.ai) und lade die Version f√ºr dein Betriebssystem herunter:

- Windows: `LM-Studio-Setup.exe`
- macOS: `LM-Studio.dmg`
- Linux: `LM-Studio.AppImage`

### 2. LM Studio installieren und starten

- **Windows**: Installer ausf√ºhren und folgen
- **macOS**: DMG √∂ffnen und in Applications ziehen
- **Linux**: AppImage ausf√ºhrbar machen (`chmod +x`) und starten

### 3. Empfohlene Modelle f√ºr Code-Dokumentation

Im LM Studio, gehe zu **Search** tab:

#### Beste Modelle (sortiert nach Qualit√§t):

1. **DeepSeek Coder 6.7B Instruct** (GGUF)
   - Suchbegriff: `TheBloke/deepseek-coder-6.7B-instruct-GGUF`
   - Empfohlen: `Q5_K_M` Quantisierung
   - RAM: ~6 GB
   - **Beste Code-Verst√§ndnis!**

2. **CodeLlama 13B Instruct** (GGUF)
   - Suchbegriff: `TheBloke/CodeLlama-13B-Instruct-GGUF`
   - Empfohlen: `Q4_K_M` Quantisierung
   - RAM: ~8 GB
   - Sehr gut f√ºr C++ Code

3. **Mistral 7B Instruct v0.2** (GGUF)
   - Suchbegriff: `TheBloke/Mistral-7B-Instruct-v0.2-GGUF`
   - Empfohlen: `Q5_K_M` Quantisierung
   - RAM: ~5 GB
   - Gute Allround-Leistung

4. **Phind CodeLlama 34B v2** (GGUF)
   - Suchbegriff: `TheBloke/Phind-CodeLlama-34B-v2-GGUF`
   - Empfohlen: `Q3_K_M` oder `Q4_K_S` Quantisierung
   - RAM: ~20 GB (Q3_K_M) oder ~25 GB (Q4_K_S)
   - **Beste Qualit√§t, aber hohe Anforderungen!**

#### Quantisierungen erkl√§rt:

- **Q8_0**: H√∂chste Qualit√§t, gr√∂√üter Speicherbedarf
- **Q5_K_M**: Guter Kompromiss (empfohlen)
- **Q4_K_M**: Niedrigerer Speicher, gute Qualit√§t
- **Q3_K_M**: Kleiner, akzeptable Qualit√§t
- **Q2_K**: Sehr klein, niedrige Qualit√§t

### 4. Modell herunterladen

1. Klicke auf **Download** beim gew√ºnschten Modell
2. W√§hle die Quantisierung (empfohlen: Q5_K_M oder Q4_K_M)
3. Warte, bis Download abgeschlossen ist

### 5. Modell laden

1. Gehe zum **Chat** tab
2. W√§hle das heruntergeladene Modell aus dem Dropdown
3. Das Modell wird geladen (kann 10-30 Sekunden dauern)

### 6. Server starten

1. Gehe zum **Developer** tab (oder **Local Server**)
2. Klicke auf **Start Server**
3. Server l√§uft standardm√§√üig auf `http://localhost:1234`
4. **Wichtig**: Notiere den exakten Modellnamen im Dropdown (z.B. `deepseek-coder-6.7b-instruct`)

## MkDocs Plugin Konfiguration

### Minimale Konfiguration

```yaml
plugins:
  - llm-autodoc:
      enabled: true
      cpp_project_path: '../your-cpp-project'
      llm_provider: 'lmstudio'
      llm_model: 'deepseek-coder-6.7b-instruct'  # Dein Modellname
      llm_base_url: 'http://localhost:1234/v1'
```

### Vollst√§ndige Konfiguration

```yaml
plugins:
  - llm-autodoc:
      # Basic Settings
      enabled: true
      cpp_project_path: '../your-cpp-project'

      # LM Studio Configuration
      llm_provider: 'lmstudio'
      llm_model: 'deepseek-coder-6.7b-instruct'  # Exakter Name aus LM Studio
      llm_base_url: 'http://localhost:1234/v1'   # Standard-Port

      # Optimization f√ºr lokale Modelle
      max_concurrent_llm_calls: 1  # LM Studio verarbeitet requests sequentiell
      enable_cache: true            # Wichtig f√ºr Performance!

      # Output
      high_level_output: 'generated'
      mid_level_output: 'generated/modules'
      detailed_level_output: 'generated/api'
```

## Verwendung

### 1. Sicherstellen, dass Server l√§uft

Im LM Studio **Developer** tab sollte stehen:
```
Server running on http://localhost:1234
```

### 2. Dokumentation generieren

```bash
mkdocs build
```

### 3. Erwartete Performance

- **DeepSeek Coder 6.7B**: ~5-15 Sekunden pro Datei
- **CodeLlama 13B**: ~10-20 Sekunden pro Datei
- **Mistral 7B**: ~5-10 Sekunden pro Datei

*Abh√§ngig von Hardware (GPU, CPU, RAM)*

## Troubleshooting

### Problem: "Connection refused"

**Symptom:**
```
Error: Failed to connect to http://localhost:1234
```

**L√∂sung:**
1. Pr√ºfe, ob LM Studio Server l√§uft (Developer tab ‚Üí Start Server)
2. Pr√ºfe Port in LM Studio Settings
3. Pr√ºfe `llm_base_url` in mkdocs.yml

### Problem: "Model not found"

**Symptom:**
```
Error: Model 'your-model' not found
```

**L√∂sung:**
1. Gehe zu LM Studio Chat tab
2. Kopiere exakten Modellnamen aus dem Dropdown
3. Update `llm_model` in mkdocs.yml

### Problem: Langsame Generation

**Symptom:**
Dokumentation dauert sehr lange

**L√∂sungen:**
1. **Kleineres Modell verwenden**:
   - Statt CodeLlama 13B ‚Üí DeepSeek Coder 6.7B
   - Niedrigere Quantisierung (Q4 statt Q5)

2. **GPU-Acceleration aktivieren**:
   - LM Studio Settings ‚Üí Hardware
   - Enable GPU Offloading
   - Mehr Layers auf GPU

3. **Cache aktivieren**:
   ```yaml
   enable_cache: true
   force_regenerate: false
   ```

4. **Selective Generation**:
   ```yaml
   generate_detailed_level: false  # Nur High + Mid
   ```

### Problem: Out of Memory

**Symptom:**
LM Studio st√ºrzt ab oder System friert ein

**L√∂sungen:**
1. **Kleineres Modell**:
   - 13B ‚Üí 7B
   - 7B ‚Üí 6.7B

2. **Niedrigere Quantisierung**:
   - Q5_K_M ‚Üí Q4_K_M
   - Q4_K_M ‚Üí Q3_K_M

3. **Context Length reduzieren**:
   - LM Studio Settings ‚Üí Advanced
   - Context Length: 4096 ‚Üí 2048

## Modell-Empfehlungen nach Hardware

### 8 GB RAM
- **DeepSeek Coder 6.7B** (Q4_K_M)
- **Mistral 7B** (Q4_K_M)

### 16 GB RAM
- **DeepSeek Coder 6.7B** (Q5_K_M) ‚≠ê **Empfohlen**
- **CodeLlama 13B** (Q4_K_M)
- **Mistral 7B** (Q5_K_M)

### 32 GB RAM
- **CodeLlama 13B** (Q5_K_M)
- **Phind CodeLlama 34B** (Q3_K_M)
- **DeepSeek Coder 33B** (Q4_K_M)

### 64+ GB RAM (mit GPU)
- **Phind CodeLlama 34B** (Q5_K_M)
- **CodeLlama 70B** (Q3_K_M)

## Tipps f√ºr beste Ergebnisse

1. **Verwende Code-spezifische Modelle**:
   - DeepSeek Coder ‚≠ê
   - CodeLlama
   - Phind CodeLlama

2. **Aktiviere GPU-Offloading**:
   - LM Studio Settings ‚Üí Hardware
   - Offload mehr Layers f√ºr bessere Performance

3. **Cache nutzen**:
   - Erste Generierung dauert l√§nger
   - Nachfolgende Builds sind viel schneller

4. **Inkrementelle Updates**:
   ```yaml
   force_regenerate: false
   ```
   Nur ge√§nderte Dateien werden neu dokumentiert

5. **Batch Processing reduzieren**:
   ```yaml
   max_concurrent_llm_calls: 1
   ```
   LM Studio verarbeitet requests am besten sequentiell

## Vergleich: LM Studio vs. Ollama

| Feature | LM Studio | Ollama |
|---------|-----------|--------|
| GUI | ‚úÖ Ja | ‚ùå Nein |
| Einfachheit | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Modell-Auswahl | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Performance | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Speicherbedarf | H√∂her | Niedriger |
| API-Kompatibilit√§t | OpenAI | OpenAI |
| Best for | Einsteiger, GUI-Fans | Fortgeschrittene, CLI-Fans |

## Weitere Ressourcen

- üìñ [LM Studio Dokumentation](https://lmstudio.ai/docs)
- üí¨ [LM Studio Discord](https://discord.gg/lmstudio)
- ü§ó [Hugging Face Modelle](https://huggingface.co/models?library=gguf)
- üìä [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

Viel Erfolg mit LM Studio! üöÄ
